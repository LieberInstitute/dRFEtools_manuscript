{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation analysis examining 4 classification models for simulated Bulk RNA-sequencing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import os,errno,dRFEtools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rpy2.robjects import r, pandas2ri, globalenv\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(directory):\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "\n",
    "def load_data(simu):\n",
    "    pandas2ri.activate()\n",
    "    globalenv[\"simu\"] = simu+1\n",
    "    r('''\n",
    "    suppressPackageStartupMessages(library(dplyr))\n",
    "    counts <- data.table::fread(paste0(\"../../_m/bulk_data/simulated_counts_\",simu,\".tsv.gz\")) %>%\n",
    "        tibble::column_to_rownames(\"V1\") %>% as.matrix\n",
    "    phenotypes <- data.table::fread(paste0(\"../../_m/bulk_data/simulated_sampleInfo_\", simu, \".tsv\")) %>%\n",
    "        tibble::column_to_rownames(\"V1\")\n",
    "    x <- edgeR::DGEList(counts=counts, samples=phenotypes)\n",
    "    x <- edgeR::calcNormFactors(x, method=\"TMM\")\n",
    "    Z <- edgeR::cpm(x, log=TRUE) %>% as.data.frame\n",
    "    ''')\n",
    "    return r['Z'].T, r[\"phenotypes\"]\n",
    "\n",
    "\n",
    "def run_oob(estimator, x_train, x_test, y_train, y_test, fold, outdir, \n",
    "            frac, step, simu):\n",
    "    features = x_train.columns\n",
    "    d, pfirst = dRFEtools.rf_rfe(estimator, x_train, y_train, features, \n",
    "                                fold, outdir, elimination_rate=0.1, RANK=True)\n",
    "    df_elim = pd.DataFrame([{'fold':fold, \"simulation\": simu,\n",
    "                             'n features':k, 'NMI score':d[k][1], \n",
    "                             'Accuracy score':d[k][2], \n",
    "                             'ROC AUC score':d[k][3]} for k in d.keys()])\n",
    "    n_features_max = max(d, key=lambda x: d[x][1])\n",
    "    try:\n",
    "        ## Max features from lowess curve\n",
    "        n_features, _ = dRFEtools.extract_max_lowess(d, frac=frac, multi=False)\n",
    "        n_redundant, _ = dRFEtools.extract_redundant_lowess(d, frac=frac, \n",
    "                                                            step_size=step, \n",
    "                                                            multi=False)\n",
    "        dRFEtools.plot_with_lowess_vline(d, fold, outdir, frac=frac,\n",
    "                                         step_size=step, multi=False)\n",
    "    except ValueError:\n",
    "        ## For errors in lowess estimate\n",
    "        n_features = n_features_max \n",
    "        n_redundant = n_features\n",
    "    ## Fit model\n",
    "    estimator.fit(x_train, y_train)\n",
    "    all_fts = estimator.predict(x_test)\n",
    "    estimator.fit(x_train.iloc[:, d[n_redundant][4]], y_train)\n",
    "    labels_pred_redundant = estimator.predict(x_test.iloc[:, d[n_redundant][4]])\n",
    "    estimator.fit(x_train.iloc[:,d[n_features][4]], y_train)\n",
    "    labels_pred = estimator.predict(x_test.iloc[:, d[n_features][4]])\n",
    "    ## Output test predictions\n",
    "    kwargs = {\"average\": \"weighted\"}\n",
    "    pd.DataFrame({'fold': fold, \"simulation\": simu, 'real': y_test, \n",
    "                  'predict_all': all_fts, 'predict_max': labels_pred, \n",
    "                  'predict_redundant': labels_pred_redundant})\\\n",
    "      .to_csv(\"%s/test_predictions.txt\" % outdir, sep='\\t', mode='a', \n",
    "              index=True, header=True if fold == 0 else False)\n",
    "    output = dict()\n",
    "    output['fold'] = fold\n",
    "    output['simulation'] = simu\n",
    "    output['n_features'] = n_features\n",
    "    output['n_redundant'] = n_redundant\n",
    "    output['n_max'] = n_features_max\n",
    "    output['train_nmi'] = dRFEtools.oob_score_nmi(estimator, y_train)\n",
    "    output['train_acc'] = dRFEtools.oob_score_accuracy(estimator, y_train)\n",
    "    output['train_roc'] = dRFEtools.oob_score_roc(estimator, y_train)\n",
    "    output['test_nmi'] = nmi(y_test, labels_pred, average_method=\"arithmetic\")\n",
    "    output['test_acc'] = accuracy_score(y_test, labels_pred)\n",
    "    output['test_roc'] = roc_auc_score(y_test, labels_pred, **kwargs)\n",
    "    metrics_df = pd.DataFrame.from_records(output, index=[simu]).reset_index().drop('index', axis=1)\n",
    "    return df_elim, metrics_df\n",
    "\n",
    "\n",
    "def run_dev(estimator, x_train, x_test, y_train, y_test, fold, outdir, \n",
    "                 frac, step, simu):\n",
    "    features = x_train.columns\n",
    "    d, pfirst = dRFEtools.dev_rfe(estimator, x_train, y_train, features, \n",
    "                                 fold, outdir, elimination_rate=0.1, RANK=True)\n",
    "    df_elim = pd.DataFrame([{'fold':fold, \"simulation\": simu,\n",
    "                             'n features':k, 'NMI score':d[k][1], \n",
    "                             'Accuracy score':d[k][2], \n",
    "                             'ROC AUC score':d[k][3]} for k in d.keys()])\n",
    "    n_features_max = max(d, key=lambda x: d[x][1])\n",
    "    try:\n",
    "        ## Max features from lowess curve\n",
    "        ### multiple classification is False by default\n",
    "        n_features, _ = dRFEtools.extract_max_lowess(d, frac=frac)\n",
    "        n_redundant, _ = dRFEtools.extract_redundant_lowess(d, frac=frac, \n",
    "                                                            step_size=step)\n",
    "        dRFEtools.plot_with_lowess_vline(d, fold, outdir, frac=frac, \n",
    "                                         step_size=step, multi=False)\n",
    "    except ValueError:\n",
    "        ## For errors in lowess estimate\n",
    "        n_features = n_features_max \n",
    "        n_redundant = n_features\n",
    "    ## Fit model\n",
    "    #x_dev, x_test, y_dev, y_test = train_test_split(x_train, y_train)\n",
    "    estimator.fit(x_train, y_train)\n",
    "    all_fts = estimator.predict(x_test)\n",
    "    estimator.fit(x_train.iloc[:, d[n_redundant][4]], y_train)\n",
    "    labels_pred_redundant = estimator.predict(x_test.iloc[:, d[n_redundant][4]])\n",
    "    estimator.fit(x_train.iloc[:,d[n_features][4]], y_train)\n",
    "    labels_pred = estimator.predict(x_test.iloc[:, d[n_features][4]])\n",
    "    ## Output test predictions\n",
    "    kwargs = {\"average\": \"weighted\"}\n",
    "    pd.DataFrame({'fold': fold, \"simulation\": simu, 'real': y_test, \n",
    "                  'predict_all': all_fts, 'predict_max': labels_pred, \n",
    "                  'predict_redundant': labels_pred_redundant})\\\n",
    "      .to_csv(\"%s/test_predictions.txt\" % outdir, sep='\\t', mode='a', index=True, \n",
    "              header=True if fold == 0 else False)\n",
    "    output = dict()\n",
    "    output['fold'] = fold\n",
    "    output['simulation'] = simu\n",
    "    output['n_features'] = n_features\n",
    "    output['n_redundant'] = n_redundant\n",
    "    output['n_max'] = n_features_max\n",
    "    output['train_nmi'] = dRFEtools.dev_score_nmi(estimator, x_train.iloc[:,d[n_features][4]], y_train)\n",
    "    output['train_acc'] = dRFEtools.dev_score_accuracy(estimator, x_train.iloc[:,d[n_features][4]], y_train)\n",
    "    output['train_roc'] = dRFEtools.dev_score_roc(estimator, x_train.iloc[:,d[n_features][4]], y_train)\n",
    "    output['test_nmi'] = nmi(y_test, labels_pred, average_method=\"arithmetic\")\n",
    "    output['test_acc'] = accuracy_score(y_test, labels_pred)\n",
    "    output['test_roc'] = roc_auc_score(y_test, labels_pred, **kwargs)\n",
    "    metrics_df = pd.DataFrame.from_records(output, index=[simu]).reset_index().drop('index', axis=1)\n",
    "    return df_elim, metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=20210930)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"lr/\"\n",
    "mkdir_p(outdir)\n",
    "cla = dRFEtools.LogisticRegression(n_jobs=-1, random_state=13, \n",
    "                                   max_iter=1000, penalty=\"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data(0)\n",
    "y = Y.Group.astype(\"category\").cat.codes\n",
    "\n",
    "fold = 1\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    fold += 1\n",
    "fold -= 1\n",
    "\n",
    "features = X_train.columns\n",
    "d, pfirst = dRFEtools.dev_rfe(cla, X_train, y_train, features, fold, \n",
    "                             outdir, elimination_rate=0.1, RANK=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_lt = []; simu_lt = []\n",
    "for simu in range(2):\n",
    "    X, Y = load_data(simu)\n",
    "    y = Y.Group.astype(\"category\").cat.codes\n",
    "    simu_out = \"%s/simulate_%d\" % (outdir, simu)\n",
    "    mkdir_p(simu_out)\n",
    "    ## default parameters\n",
    "    frac = 0.3; step=0.05; fold = 0\n",
    "    df_dict = pd.DataFrame(); output = pd.DataFrame()\n",
    "    start = time()\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        df_elim, metrics_df = run_dev(cla, X_train, X_test, y_train, y_test, fold, \n",
    "                                      simu_out, frac, step, simu)\n",
    "        df_dict = pd.concat([df_dict, df_elim], axis=0)\n",
    "        output = pd.concat([output, metrics_df], axis=0)\n",
    "        fold += 1\n",
    "    end = time()\n",
    "    df_dict.to_csv(\"%s/dRFE_simulation_elimination.txt\" % outdir,\n",
    "                   sep='\\t', mode='a', index=False, \n",
    "                   header=True if simu == 0 else False)\n",
    "    output.to_csv(\"%s/dRFE_simulation_metrics.txt\" % outdir,\n",
    "                  sep='\\t', mode='a', index=False, \n",
    "                  header=True if simu == 0 else False)\n",
    "    cpu_lt.append(end - start)\n",
    "    simu_lt.append(simu)\n",
    "pd.DataFrame({\"Simulation\": simu_lt, \"CPU Time\": cpu_lt})\\\n",
    "  .to_csv(\"%s/simulation_time.csv\" % outdir, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"svc/\"\n",
    "mkdir_p(outdir)\n",
    "cla = dRFEtools.LinearSVC(random_state=13, max_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data(0)\n",
    "y = Y.Group.astype(\"category\").cat.codes\n",
    "\n",
    "fold = 1\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    fold += 1\n",
    "fold -= 1\n",
    "\n",
    "features = X_train.columns\n",
    "d, pfirst = dRFEtools.dev_rfe(cla, X_train, y_train, features, fold, \n",
    "                             outdir, elimination_rate=0.1, RANK=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frac in [0.2, 0.25, 0.3, 0.35]:\n",
    "    dRFEtools.optimize_lowess_plot(d, fold, outdir, frac=frac, step_size=0.05, \n",
    "                                   classify=True, save_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_lt = []; simu_lt = []\n",
    "for simu in range(2):\n",
    "    X, Y = load_data(simu)\n",
    "    y = Y.Group.astype(\"category\").cat.codes\n",
    "    simu_out = \"%s/simulate_%d\" % (outdir, simu)\n",
    "    mkdir_p(simu_out)\n",
    "    ## default parameters\n",
    "    frac = 0.3; step=0.05; fold = 0\n",
    "    df_dict = pd.DataFrame(); output = pd.DataFrame()\n",
    "    start = time()\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        df_elim, metrics_df = run_dev(cla, X_train, X_test, y_train, y_test, fold, \n",
    "                                      simu_out, frac, step, simu)\n",
    "        df_dict = pd.concat([df_dict, df_elim], axis=0)\n",
    "        output = pd.concat([output, metrics_df], axis=0)\n",
    "        fold += 1\n",
    "    end = time()\n",
    "    df_dict.to_csv(\"%s/dRFE_simulation_elimination.txt\" % outdir,\n",
    "                   sep='\\t', mode='a', index=False, \n",
    "                   header=True if simu == 0 else False)\n",
    "    output.to_csv(\"%s/dRFE_simulation_metrics.txt\" % outdir,\n",
    "                  sep='\\t', mode='a', index=False, \n",
    "                  header=True if simu == 0 else False)\n",
    "    cpu_lt.append(end - start)\n",
    "    simu_lt.append(simu)\n",
    "pd.DataFrame({\"Simulation\": simu_lt, \"CPU Time\": cpu_lt})\\\n",
    "  .to_csv(\"%s/simulation_time.csv\" % outdir, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"sgd/\"\n",
    "mkdir_p(outdir)\n",
    "cla = dRFEtools.SGDClassifier(random_state=13, loss=\"perceptron\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data(0)\n",
    "y = Y.Group.astype(\"category\").cat.codes\n",
    "\n",
    "fold = 1\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    fold += 1\n",
    "fold -= 1\n",
    "\n",
    "features = X_train.columns\n",
    "d, pfirst = dRFEtools.dev_rfe(cla, X_train, y_train, features, fold, \n",
    "                             outdir, elimination_rate=0.1, RANK=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frac in [0.2, 0.25, 0.3, 0.35]:\n",
    "    dRFEtools.optimize_lowess_plot(d, fold, outdir, frac=frac, step_size=0.05, \n",
    "                                   classify=True, save_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_lt = []; simu_lt = []\n",
    "for simu in range(2):\n",
    "    X, Y = load_data(simu)\n",
    "    y = Y.Group.astype(\"category\").cat.codes\n",
    "    simu_out = \"%s/simulate_%d\" % (outdir, simu)\n",
    "    mkdir_p(simu_out)\n",
    "    ## default parameters\n",
    "    frac = 0.3; step=0.05; fold = 0\n",
    "    df_dict = pd.DataFrame(); output = pd.DataFrame()\n",
    "    start = time()\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        df_elim, metrics_df = run_dev(cla, X_train, X_test, y_train, y_test, fold, \n",
    "                                      simu_out, frac, step, simu)\n",
    "        df_dict = pd.concat([df_dict, df_elim], axis=0)\n",
    "        output = pd.concat([output, metrics_df], axis=0)\n",
    "        fold += 1\n",
    "    end = time()\n",
    "    df_dict.to_csv(\"%s/dRFE_simulation_elimination.txt\" % outdir,\n",
    "                   sep='\\t', mode='a', index=False, \n",
    "                   header=True if simu == 0 else False)\n",
    "    output.to_csv(\"%s/dRFE_simulation_metrics.txt\" % outdir,\n",
    "                  sep='\\t', mode='a', index=False, \n",
    "                  header=True if simu == 0 else False)\n",
    "    cpu_lt.append(end - start)\n",
    "    simu_lt.append(simu)\n",
    "pd.DataFrame({\"Simulation\": simu_lt, \"CPU Time\": cpu_lt})\\\n",
    "  .to_csv(\"%s/simulation_time.csv\" % outdir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'rf/'\n",
    "mkdir_p(outdir)\n",
    "cla = dRFEtools.RandomForestClassifier(n_estimators=100, oob_score=True, \n",
    "                                      n_jobs=-1, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(0)\n",
    "fold = 1\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    fold += 1\n",
    "fold -= 1\n",
    "\n",
    "features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, pfirst = dRFEtools.rf_rfe(cla, X_train.values, y_train.values, features, \n",
    "                            fold, outdir, elimination_rate=0.2, RANK=False)\n",
    "\n",
    "for frac in [0.2, 0.25, 0.3, 0.35]:\n",
    "    dRFEtools.optimize_lowess_plot(d, fold, outdir, frac=frac, step_size=0.05, \n",
    "                                   classify=True, save_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step_size in [0.01, 0.02, 0.03, 0.04, 0.05, 0.1]:\n",
    "    dRFEtools.optimize_lowess_plot(d, fold, outdir, frac=0.3, step_size=step_size, \n",
    "                                   classify=True, save_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directory\n",
    "outdir = 'classification_simu/'\n",
    "mkdir_p(outdir)\n",
    "\n",
    "# Create a dataset with only 10 informative features\n",
    "\n",
    "cla = RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RaFFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raffe.extract_max_lowess(d, frac=0.35))\n",
    "raffe.extract_redundant_lowess(d, frac=0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to run RaFFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_raffe_cla(estimator, x_train, x_test, y_train, y_test, fold, outdir):\n",
    "    features = [\"feature_%d\" % x for x in range(x_train.shape[1])]\n",
    "    d, pfirst = raffe.feature_elimination(estimator, x_train, y_train, \n",
    "                                          np.array(features), \n",
    "                                          fold, outdir, \n",
    "                                          elimination_rate=0.1, \n",
    "                                          RANK=True)\n",
    "    df_elim = pd.DataFrame([{'fold':fold,\n",
    "                             'n features':k,\n",
    "                             'normalized mutual information':d[k][1], \n",
    "                             'accuracy':d[k][2], \n",
    "                             'ROC AUC':d[k][3]} for k in d.keys()])\n",
    "    n_features_max = max(d, key=lambda x: d[x][1])\n",
    "    try:\n",
    "        n_features,_ = raffe.extract_max_lowess(d, frac=0.35)\n",
    "        n_redundant,_ = raffe.extract_redundant_lowess(d, frac=0.35)\n",
    "        raffe.plot_with_lowess_vline(d, fold, outdir,\n",
    "                                     classify=True)\n",
    "        raffe.optimize_lowess_plot(d, fold, outdir, frac=0.35, step_size=0.05, \n",
    "                                   classify=True, save_plot=True)\n",
    "    except ValueError:\n",
    "        n_features = n_features_max \n",
    "    estimator.fit(x_train[:,d[n_features][4]], y_train)\n",
    "    labels_pred = estimator.predict(x_test[:, d[n_features][4]])\n",
    "    metrics_df = pd.DataFrame({'n_features_max': n_features_max, \n",
    "                               'n_features': n_features, \n",
    "                               'n_redundant': n_redundant,\n",
    "                               'train_acc':raffe.oob_score_accuracy(estimator, y_train), \n",
    "                               'train_nmi':raffe.oob_score_nmi(estimator, y_train),\n",
    "                               'train_roc':raffe.oob_score_roc(estimator, y_train), \n",
    "                               'test_acc':accuracy_score(y_test, labels_pred), \n",
    "                               'test_nmi':nmi(y_test, labels_pred,\n",
    "                                              average_method='arithmetic'), \n",
    "                               'test_roc':roc_auc_score(y_test, labels_pred)}, \n",
    "                              index=[fold])\n",
    "    return df_elim, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "df_dict = pd.DataFrame()\n",
    "output = pd.DataFrame()\n",
    "fold = 1\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    df_elim, metrics_df = run_raffe_cla(cla, X_train, X_test, y_train, \n",
    "                                        y_test, fold, outdir)\n",
    "    df_dict = pd.concat([df_dict, df_elim], axis=0)\n",
    "    output = pd.concat([output, metrics_df.reset_index()], axis=0)\n",
    "    fold += 1\n",
    "end = time()\n",
    "print(f\"Runtime of the program is {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = output.set_index('index').loc[:, 'n_features'].mean()\n",
    "lo = output.set_index('index').loc[:, 'n_features_max'].mean()\n",
    "output.set_index('index').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.melt(df_dict, id_vars=['fold', 'n features'], \n",
    "              value_vars=['normalized mutual information', 'accuracy', 'ROC AUC'],\n",
    "              var_name='Metrics', value_name='Score')\n",
    "\n",
    "gg = ggplot(dft, aes(x='n features', y='Score', color='Metrics')) +\\\n",
    "    geom_jitter(size=1, alpha=0.6) + facet_wrap('~Metrics') +\\\n",
    "    geom_vline(xintercept=li, color='black', linetype='dashed') +\\\n",
    "    scale_x_log10() + theme_classic() + theme(legend_position=\"top\")\n",
    "save_plot(gg, '%s/raffe_feature_selection' % outdir, 12, 4)\n",
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
